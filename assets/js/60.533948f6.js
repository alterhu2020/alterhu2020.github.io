(window.webpackJsonp=window.webpackJsonp||[]).push([[60],{314:function(t,s,a){"use strict";a.r(s);var n=a(9),e=Object(n.a)({},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"scrapy爬虫配置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scrapy爬虫配置","aria-hidden":"true"}},[t._v("#")]),t._v(" Scrapy爬虫配置")]),t._v(" "),a("ClientOnly",[a("in-article-adsense",{attrs:{"ins-style":"display:block; text-align:center;","data-ad-slot":"7727965566"}})],1),t._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#scrapy爬虫配置"}},[t._v("Scrapy爬虫配置")])]),a("li",[a("a",{attrs:{href:"#_1-开发环境"}},[t._v("1. 开发环境")]),a("ul",[a("li",[a("a",{attrs:{href:"#_1-1-需要安装如下必需的开发包：-pipenv-类库包如下："}},[t._v("1.1 需要安装如下必需的开发包：pipenv,类库包如下：")])]),a("li",[a("a",{attrs:{href:"#操作步骤"}},[t._v("操作步骤")])]),a("li",[a("a",{attrs:{href:"#_1-2-配置对应的-scrapy-cfg-文件中的-scrapyd-服务器-如果使用下面的gerapy则不需要配置这个部分-该部分主要是为了-scrapyd-deploy-使用："}},[t._v("1.2 配置对应的scrapy.cfg文件中的scrapyd服务器(如果使用下面的gerapy则不需要配置这个部分),该部分主要是为了scrapyd-deploy使用：")])])])]),a("li",[a("a",{attrs:{href:"#_2-分布式脚本执行部署环境"}},[t._v("2. 分布式脚本执行部署环境")]),a("ul",[a("li",[a("a",{attrs:{href:"#_2-1-gerapy-服务器环境配置"}},[t._v("2.1 gerapy服务器环境配置")])]),a("li",[a("a",{attrs:{href:"#_2-2-scrapyd-脚本执行机器环境配置"}},[t._v("2.2 scrapyd脚本执行机器环境配置")])])])]),a("li",[a("a",{attrs:{href:"#问题"}},[t._v("问题")])])])]),a("p"),t._v(" "),a("h2",{attrs:{id:"_1-开发环境"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-开发环境","aria-hidden":"true"}},[t._v("#")]),t._v(" 1. 开发环境")]),t._v(" "),a("h3",{attrs:{id:"_1-1-需要安装如下必需的开发包：pipenv-类库包如下："}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-需要安装如下必需的开发包：pipenv-类库包如下：","aria-hidden":"true"}},[t._v("#")]),t._v(" 1.1 需要安装如下必需的开发包："),a("code",[t._v("pipenv")]),t._v(",类库包如下：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("$ pip install pipenv\n$ pipenv install scrapy scrapyd-client\n$ pipenv install requests pymysql beautifulsoup4 lxml js2py selenium  \n")])])]),a("ul",[a("li",[t._v("数据库主机、数据库名称、用户名、密码等信息在"),a("code",[t._v("settings.py")]),t._v("文件中配置；")]),t._v(" "),a("li",[t._v("配置好数据库后，cmd进入程序所在目录，运行`scrapy crawl 项目名称即可；")]),t._v(" "),a("li",[t._v("使用"),a("code",[t._v("scrapyd-client")]),t._v("中的"),a("code",[t._v("scrapyd-deploy")]),t._v("命令行工具可以将脚本部署到指定的"),a("code",[t._v("scrapyd")]),t._v("服务上；")])]),t._v(" "),a("h3",{attrs:{id:"操作步骤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#操作步骤","aria-hidden":"true"}},[t._v("#")]),t._v(" 操作步骤")]),t._v(" "),a("p",[t._v("1.创建项目")]),t._v(" "),a("p",[t._v("在开始爬取之前，我们必须创建一个新的Scrapy项目，我这里命名为jianshu_article。打开Mac终端，cd到你打算存储代码的目录中，运行下列命令:")]),t._v(" "),a("div",{staticClass:"language-sh extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[t._v("// 安装scrapy\npip "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" scrapy\nscrapy --help\n//Mac终端运行如下命令：\nscrapy startproject spider_pingbook\n")])])]),a("p",[t._v("2.创建爬虫程序")]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("//cd到上面创建的文件目录\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("cd")]),t._v(" spider_pingbook\n//创建爬虫程序\nscrapy genspider jianshu jianshu.com\n/*\n文件说明：\n  scrapy.cfg  项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）\n  items.py    设置数据存储模型，用于结构化数据，如：Django的Model\n  pipelines    数据处理行为，如：一般结构化的数据持久化\n  settings.py 配置文件，如：USER_AGENT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("模拟浏览器，应对网站反爬"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("，递归的层数、并发数，延迟下载等\n  spiders      爬虫目录，如：创建文件，编写爬虫规则\n*/\n")])])]),a("p",[t._v("为了方便编写程序，我们用Pycharm打开项目，执行完上面的命令程序会自动创建目录及文件，其中生成了一个jianshu.py的文件，后面我们主要逻辑都将写在此文件中。")]),t._v(" "),a("p",[t._v("3.设置数据模型\n双击items.py文件。\n找到你想爬取的简书作者首页，如我自己的首页https://www.jianshu.com/u/6b14223f1b58，用谷歌浏览器打开，空白处鼠标右击，单击“检查”进入控制台开发者模式：\n通过分析网页源码，我们大概需要这些内容：")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding: utf-8 -*-")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define here the models for your scraped items")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# See documentation in:")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# https://doc.scrapy.org/en/latest/topics/items.html")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JianshuArticalItem")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    avatar "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#头像")]),t._v("\n    nickname "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#昵称")]),t._v("\n    time "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#发表时间")]),t._v("\n    wrap_img "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#封面（缺省值）")]),t._v("\n    title "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#标题")]),t._v("\n    abstract "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#正文部分显示")]),t._v("\n    read "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#查看人数")]),t._v("\n    comments "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#评论数")]),t._v("\n    like "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#喜欢（点赞）")]),t._v("\n    detail "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#文章详情url")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n")])])]),a("p",[t._v("如此数据模型就创建好了，后面运行爬虫的时候，我得到的数据将存进模型对应的位置。")]),t._v(" "),a("p",[t._v("4.分析网页源码，编写爬虫\n推荐一款chrome的xpath自动选择生成工具"),a("a",{attrs:{href:"https://www.crx4chrome.com/extensions/ljngjbnaijcbncmcnjfhigebomdlkcjo/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ChroPath"),a("OutboundLink")],1),t._v("这里给出XPath表达式的例子及对应的含义:")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("/html/head/title")]),t._v(": 选择HTML文档中 "),a("head",[t._v(" 标签内的 "),a("title",[t._v(" 元素")])])]),t._v(" "),a("li",[a("code",[t._v("/html/head/title/text()")]),t._v(": 选择上面提到的 "),a("title",[t._v(" 元素的文字")])]),t._v(" "),a("li",[a("code",[t._v("//td")]),t._v(" "),a("code",[t._v("//li")]),t._v(": 选择所有的 "),a("td",[t._v(" 元素")])]),t._v(" "),a("li",[a("code",[t._v('//div[@class="mine"]')]),t._v(': 选择所有具有 class="mine" 属性的 div 元素')])]),t._v(" "),a("p",[t._v("上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。 如果您想了解的更多，我们推荐 这篇XPath教程 。\n通过上面的介绍，相信你可以做接下来的爬虫工作了，下面贴上jianshu.py的全部代码，以供参考：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding: utf-8 -*-")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" jianshu_article"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" JianshuArticleItem\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JianshuSpider")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Spider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'jianshu'")]),t._v("\n    allowed_domains "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'jianshu.com'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"1b4c832fb2ca"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#替换此用户ID可获取你需要的数据，或者放开下一行的注释")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#user_id = input('请输入作者id：\\n')")]),t._v("\n    url "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://www.jianshu.com/u/{0}?page=1"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    start_urls "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [关注,粉丝,文章]")]),t._v("\n        a "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('\'//div[@class="main-top"]/div[@class="info"]/ul/li/div/a/p/text()\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [字数,收获喜欢]")]),t._v("\n        b "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('\'//div[@class="main-top"]/div[@class="info"]/ul/li/div/p/text()\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 大头像")]),t._v("\n        c "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('\'//div[@class="main-top"]/a[@class="avatar"]/img/@src\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用户名")]),t._v("\n        d "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('\'//div[@class="main-top"]/div[@class="title"]/a/text()\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 性别")]),t._v("\n        e "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('\'//div[@class="main-top"]/div[@class="title"]/i/@class\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取文章总数，计算页数。（简书网站默认每页是9组数据）")]),t._v("\n        temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"总共"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"页"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        base_url "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://www.jianshu.com/u/{0}?page={1}"')]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            i "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" i  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#理论上正序1~count就是按顺序获取的，但是获取的数据是倒置的，所以我们获取count~1的数据，得到的数组就是按照网页形式1~count页码排序的了")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("base_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dont_filter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse_page"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#迭代返回每页的内容")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse_page")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" sel "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'//div[@id=\"list-container\"]/ul/li'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            item "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" JianshuArticleItem"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wrap_img'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a/img/@src'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'avatar'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div//a[@class=\"avatar\"]/img/@src'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'nickname'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div//a[@class=\"nickname\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'time'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div//span[@class=\"time\"]/@data-shared-at'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div/a[@class=\"title\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'abstract'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div/p[@class=\"abstract\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'read'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div/div[@class=\"meta\"]/a[1]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'comments'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div/div[@class=\"meta\"]/a[2]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'like'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div/div[@class=\"meta\"]/span/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'detail'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'div/a[@class=\"title\"]/@href'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" item\n\n至此爬虫代码编写完毕，如果要把获取的数据保存下来，你可以终端执行如下命令：\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n此命令用于把爬取的数据保存为json文件格式，当然你也可以保存为别的文件格式。\nScrapy官方列出的文件格式有如下几种："),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'json'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'jsonlines'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'jl'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xml'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marshal'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pickle'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("。\n温馨提示：如果要再次爬取，最好换一个文件名或者清空数据再爬取，因为第二还是写入上一个文件，数据不会覆盖，\n会堆积在上次获取的下面，造成json文件格式报错。\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("\nscrapy crawl jianshu "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("o data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n")])])]),a("p",[t._v("程序执行完后，我们可以在文件目录看到新生成的data.json文件，双击可以看到我们要获取的全部数据：")]),t._v(" "),a("h3",{attrs:{id:"_1-2-配置对应的scrapy-cfg文件中的scrapyd服务器-如果使用下面的gerapy则不需要配置这个部分-该部分主要是为了scrapyd-deploy使用："}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-配置对应的scrapy-cfg文件中的scrapyd服务器-如果使用下面的gerapy则不需要配置这个部分-该部分主要是为了scrapyd-deploy使用：","aria-hidden":"true"}},[t._v("#")]),t._v(" 1.2 配置对应的"),a("code",[t._v("scrapy.cfg")]),t._v("文件中的"),a("code",[t._v("scrapyd")]),t._v("服务器(如果使用下面的gerapy则不需要配置这个部分),该部分主要是为了"),a("code",[t._v("scrapyd-deploy")]),t._v("使用：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("[deploy:cvr_news]\n# url = http://localhost:6800/\n# project = spider_test\n# username = deployer\n# password = eaafbbdbe1494810b48a90651xe3452cd95f\n")])])]),a("h2",{attrs:{id:"_2-分布式脚本执行部署环境"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-分布式脚本执行部署环境","aria-hidden":"true"}},[t._v("#")]),t._v(" 2. 分布式脚本执行部署环境")]),t._v(" "),a("blockquote",[a("blockquote",[a("blockquote",[a("p",[t._v("参考文档: "),a("a",{attrs:{href:"https://blog.csdn.net/qq_38003892/article/details/80427278",target:"_blank",rel:"noopener noreferrer"}},[t._v("scrapy部署， Gerapy 分布式爬虫管理部署使用"),a("OutboundLink")],1)])])])]),t._v(" "),a("blockquote",[a("blockquote",[a("blockquote",[a("p",[t._v("参考文档： "),a("a",{attrs:{href:"https://www.cnblogs.com/sxqjava/p/10037731.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("gerapy+scrapyd组合管理分布式爬虫"),a("OutboundLink")],1)])])])]),t._v(" "),a("h3",{attrs:{id:"_2-1-gerapy服务器环境配置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-gerapy服务器环境配置","aria-hidden":"true"}},[t._v("#")]),t._v(" 2.1 "),a("code",[t._v("gerapy")]),t._v("服务器环境配置")]),t._v(" "),a("ul",[a("li",[t._v("安装"),a("code",[t._v("scrapy")]),t._v("部署服务,也是一个远程服务。是运行"),a("code",[t._v("scrapy")]),t._v("爬虫的服务端程序,它支持以"),a("code",[t._v("http")]),t._v("接口命令方式发布、删除、启动、停止爬虫程序。")]),t._v(" "),a("li",[t._v("在电脑任意位置新建一个文件夹,打开cmd，进入到这个文件夹下，输入命令"),a("code",[t._v("gerapy init")]),t._v(".安装"),a("code",[t._v("Gerapy")]),t._v(".\n初始化完成后会生成一个文件夹"),a("code",[t._v("gerapy")]),t._v("，该文件夹下面会生成一个"),a("code",[t._v("projects")]),t._v("文件夹.进入到该创建的"),a("code",[t._v("gerapy")]),t._v("文件夹下，再输入"),a("code",[t._v("gerapy migrate")]),t._v("完成"),a("code",[t._v("gerapy")]),t._v("初始化工作. 将scrapy脚本项目放到"),a("code",[t._v("projects")]),t._v("目录下,利用"),a("code",[t._v("gerapy runserver")]),t._v("，启动"),a("code",[t._v("gerapy")]),t._v(". 刷新即可看到部署的脚本。")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("$ mkdir spider-gerapy\n$ cd spider-gerapy\n$ pip install gerapy\n$ gerapy init\n$ cd gerapy\n$ gerapy migrate \n初始化数据库\n$ gerapy createsuperuser\n创建超级用户，用于登录界面\n$ gerapy runserver\n后台静默运行gerapy服务,注意一定要切换到新创建的gerapy目录下面\n$ nohup gerapy runserver 0.0.0.0:6000 &\n")])])]),a("h4",{attrs:{id:"安装问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#安装问题","aria-hidden":"true"}},[t._v("#")]),t._v(" "),a("strong",[t._v("安装问题")])]),t._v(" "),a("ol",[a("li",[t._v("无法安装"),a("code",[t._v("gevent")]),t._v(",直接下载编译好的安装包： https://www.lfd.uci.edu/~gohlke/pythonlib")])]),t._v(" "),a("h3",{attrs:{id:"_2-2-scrapyd脚本执行机器环境配置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-scrapyd脚本执行机器环境配置","aria-hidden":"true"}},[t._v("#")]),t._v(" 2.2 "),a("code",[t._v("scrapyd")]),t._v("脚本执行机器环境配置")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("scrapyd")]),t._v("不需要设置目录，可以同时管理多个爬虫,每个爬虫还可以有多个版本：")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('$ pip install scrapyd\n一般安装在类似目录： `/usr/local/lib/python3.9/site-packages/scrapyd`\n$ find / -name "default_scrapyd.conf"\n$ nano default_scrapyd.conf\n1. 修改scrapyd服务的端口号，默认端口是`6800`\n2. 设置远程访问端口可以放行（`bind_address`由`127.0.0.1`改成0.0.0.0）\n3. 修改日志存放目录\n')])])]),a("p",[t._v("上面提到配置，需要修改的配置文件内容如下：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("eggs_dir    = /www/spider/eggs\nlogs_dir    = /logs\nitems_dir   = /www/spider/items\n\nbind_address = 0.0.0.0\nhttp_port   = 6800\n\nusername = deployer\npassword = eaafbbdbe1494810b48a9065152cd95f245dz\n")])])]),a("ul",[a("li",[t._v("上面的配置修改成功后执行以下脚本运行"),a("code",[t._v("scrapyd")]),t._v("后台启动服务：")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("$ nohup scrapyd &\n")])])]),a("ul",[a("li",[t._v("脚本运行端需要安装对应的"),a("code",[t._v("scrapy")]),t._v("开发环境中提到的所有库，执行如下命令安装:")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("pip install scrapy requests pymysql beautifulsoup4 lxml js2py selenium   \n")])])]),a("p",[t._v("所有的安装包默认安装在目录: "),a("code",[t._v("/usr/local/lib/python3.8/site-packages")])]),t._v(" "),a("h2",{attrs:{id:"问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#问题","aria-hidden":"true"}},[t._v("#")]),t._v(" 问题")]),t._v(" "),a("ol",[a("li",[t._v("在"),a("code",[t._v("gerapy")]),t._v("安装中安装的lxml会出现错误： make sure the development packages of libxml2 and libxslt are installed")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("sudo apt-get install libxml2-dev libxslt-dev\n")])])]),a("ol",{attrs:{start:"2"}},[a("li",[t._v("执行"),a("code",[t._v("gerapy init")]),t._v("命令出现错误：")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("# gerapy init\n:0: UserWarning: You do not have a working installation of the service_identity module: 'cannot import name 'opentype''.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\nInitialized workspace gerapy\n\n")])])]),a("p",[t._v("原因是：本机上的service_identity模块太老旧，而通过install安装的时候不会更新到最新版本\n解决方法：")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("强制升级,执行命令:  "),a("code",[t._v("pip install service_identity --force --upgrade")])])]),t._v(" "),a("li",[a("p",[t._v("或者是找到最新版的安装包进行手动安装，最新包下载地址: "),a("code",[t._v("https://pypi.org/project/service_identity/#files")]),t._v(",下载对应的whl文件安装即可。")])])]),t._v(" "),a("ol",{attrs:{start:"3"}},[a("li",[t._v("执行"),a("code",[t._v("pipenv install")]),t._v("出现错误：ImportError: cannot import name 'Mapping' from 'collections'")])]),t._v(" "),a("p",[t._v("原因是执行的包错误，重新安装即可")]),t._v(" "),a("ol",{attrs:{start:"4"}},[a("li",[a("p",[t._v("在gerapy中添加机器报错: "),a("code",[t._v("the JSON object must be str, not 'bytes'")]),t._v("\n可能是对应的scrapyd服务没有启动")])]),t._v(" "),a("li",[a("p",[t._v("执行scrapyd命令出错: "),a("code",[t._v("Failed to load application: No module named '_sqlite3'")])])])]),t._v(" "),a("p",[t._v("原因是python采用编译安装的，导致没有加载对应的sqlite模块，重新编译安装加载sqlite模块，命令如下:")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("./configure --enable-optimizations --enable-ipv6 --enable-loadable-sqlite-extensions\n")])])]),a("ol",{attrs:{start:"6"}},[a("li",[t._v("执行"),a("code",[t._v("pip -V")]),t._v("出错: "),a("code",[t._v("ModuleNotFoundError: No module named 'pip._internal.cli.main'")])])]),t._v(" "),a("p",[t._v("解决方法，修复pip，执行命令: "),a("code",[t._v("python -m pip install --upgrade pip")]),t._v("，或者如下命令:")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --force-reinstall\n")])])]),a("ol",{attrs:{start:"7"}},[a("li",[t._v("执行命令: "),a("code",[t._v("sudo add-apt-repository ppa:deadsnakes/ppa")]),t._v(" ,报错："),a("code",[t._v("add-apt-repository gpg: keyserver receive failed: No dirmngr")]),t._v(",执行如下命令安装：dirmngr:")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("sudo apt install dirmngr\n")])])]),a("p",[t._v("8.执行"),a("code",[t._v("apt update")]),t._v("命令报错:"),a("code",[t._v("Updating from such a repository can't be done securely, and is therefore disabled by default")]),t._v(",执行如下命令更新包：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("$ sudo apt-get update --allow-unauthenticated\n")])])]),a("ol",{attrs:{start:"9"}},[a("li",[a("p",[t._v("安装scrapy中twisted安装报错\n解决方法，切换到目录： https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted，直接下载对应的whl包")])]),t._v(" "),a("li",[a("p",[t._v("如何设置scrapy的默认的user-agent和proxy代理\n在脚本目录下方有一个配置文件: "),a("code",[t._v("settings.py")]),t._v(", 如下配置：")])])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("DOWNLOADER_MIDDLEWARES "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'spider_yanzhi.middlewares.UserAgentMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("401")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'spider_yanzhi.middlewares.CookiesMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("402")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("p",[t._v("第二个参数可以参考"),a("code",[t._v("DOWNLOADER_MIDDLEWARES_BASE")]),t._v("里面的默认数值：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("350")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("400")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.retry.RetryMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("550")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("560")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("580")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("590")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("600")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("700")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("750")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.stats.DownloaderStats'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("850")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("900")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n")])])])],1)},[],!1,null,null,null);s.default=e.exports}}]);